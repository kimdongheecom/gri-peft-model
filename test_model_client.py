# test_model_client.py
# KoAlpaca 5.8B ëª¨ë¸ í…ŒìŠ¤íŠ¸ í´ë¼ì´ì–¸íŠ¸ (RTX 5060 ìµœì í™”)

import os
import logging
import traceback
import time
import json
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# RTX 5060 (Ada Lovelace) í˜¸í™˜ì„± ì„¤ì •
os.environ["TORCH_CUDA_ARCH_LIST"] = "9.0"
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
os.environ["TORCH_ALLOW_TF32_CUBLAS_OVERRIDE"] = "1"

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# --- ì„¤ì • ---
MODEL_ID = "beomi/KoAlpaca-Polyglot-5.8B"
MODEL_PATH = Path(__file__).parent / "models" / MODEL_ID
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

class KoAlpacaModelTester:
    """KoAlpaca 5.8B ëª¨ë¸ ì§ì ‘ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.device = DEVICE
        logger.info(f"ModelTester ì´ˆê¸°í™” (Device: {self.device})")
        
    def check_model_exists(self) -> bool:
        """ëª¨ë¸ì´ ë¡œì»¬ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        config_file = MODEL_PATH / "config.json"
        model_files = list(MODEL_PATH.glob("*.safetensors")) if MODEL_PATH.exists() else []
        
        exists = config_file.exists() and len(model_files) > 0
        if exists:
            logger.info(f"âœ… ëª¨ë¸ ë°œê²¬: {MODEL_PATH}")
            logger.info(f"   - ì„¤ì • íŒŒì¼: {config_file}")
            logger.info(f"   - ëª¨ë¸ íŒŒì¼ ìˆ˜: {len(model_files)}")
        else:
            logger.error(f"âŒ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {MODEL_PATH}")
            logger.error("ğŸ’¡ 'python download_model.py'ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        
        return exists
    
    def load_model(self):
        """ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œ"""
        if not self.check_model_exists():
            raise FileNotFoundError("ëª¨ë¸ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. download_model.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        
        try:
            logger.info("ğŸ§  KoAlpaca 5.8B ëª¨ë¸ ë¡œë”© ì‹œì‘...")
            
            # RTX 5060 í˜¸í™˜ì„±ì„ ìœ„í•œ ì„¤ì •
            self.model = AutoModelForCausalLM.from_pretrained(
                MODEL_PATH,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                use_safetensors=True,
                attn_implementation="eager",  # Flash Attention ë¹„í™œì„±í™”
            )
            
            self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
            
            # íŒ¨ë”© í† í° ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            logger.info("âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ!")
            
            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            if torch.cuda.is_available():
                memory_allocated = torch.cuda.memory_allocated() / 1024**3
                logger.info(f"ğŸ”¥ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_allocated:.2f} GB")
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            raise
    
    def generate_response(self, prompt: str, max_new_tokens: int = 256) -> str:
        """í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±"""
        if not self.model or not self.tokenizer:
            raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # KoAlpaca ëŒ€í™”í˜• í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        conversation_prompt = f"""### ì§ˆë¬¸: {prompt}

### ë‹µë³€:"""
        
        logger.info(f"ğŸ¤– ì‘ë‹µ ìƒì„± ì¤‘... (ìµœëŒ€ {max_new_tokens} í† í°)")
        
        try:
            inputs = self.tokenizer(conversation_prompt, return_tensors="pt", add_special_tokens=True)
            if 'token_type_ids' in inputs:
                del inputs['token_type_ids']
            inputs = inputs.to(self.device)
            
            start_time = time.time()
            
            with torch.no_grad():
                try:
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=max_new_tokens,
                        do_sample=True,
                        temperature=0.7,
                        top_p=0.9,
                        repetition_penalty=1.1,
                        pad_token_id=self.tokenizer.eos_token_id,
                        eos_token_id=self.tokenizer.eos_token_id,
                        use_cache=True,
                        output_attentions=False,
                        output_hidden_states=False,
                    )
                except RuntimeError as e:
                    if "no kernel image is available" in str(e):
                        logger.warning("CUDA ì»¤ë„ í˜¸í™˜ì„± ë¬¸ì œ ê°ì§€. ì•ˆì „ ëª¨ë“œë¡œ ì¬ì‹œë„...")
                        outputs = self.model.generate(
                            **inputs,
                            max_new_tokens=min(max_new_tokens, 50),
                            do_sample=False,
                            pad_token_id=self.tokenizer.eos_token_id,
                            eos_token_id=self.tokenizer.eos_token_id,
                            use_cache=False,
                        )
                    else:
                        raise
            
            end_time = time.time()
            generation_time = end_time - start_time
            
            response_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ë‹µë³€ ë¶€ë¶„ë§Œ ì¶”ì¶œ
            answer_start = response_text.find("### ë‹µë³€:") + len("### ë‹µë³€:")
            answer = response_text[answer_start:].strip()
            
            # ë‹¤ìŒ ì§ˆë¬¸ì´ ì‹œì‘ë˜ë©´ ê·¸ ì „ê¹Œì§€ë§Œ ë°˜í™˜
            if "### ì§ˆë¬¸:" in answer:
                answer = answer.split("### ì§ˆë¬¸:")[0].strip()
            
            logger.info(f"â±ï¸ ìƒì„± ì‹œê°„: {generation_time:.2f}ì´ˆ")
            
            return answer
            
        except Exception as e:
            logger.error(f"âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            raise

def test_model_connection(tester: KoAlpacaModelTester):
    """ëª¨ë¸ ì—°ê²° ë° ë¡œë”© í…ŒìŠ¤íŠ¸"""
    try:
        logger.info("ğŸ”§ ëª¨ë¸ ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        tester.load_model()
        logger.info("âœ… ëª¨ë¸ ì—°ê²° ì„±ê³µ!")
        return True
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ì—°ê²° ì‹¤íŒ¨: {e}")
        return False

def run_interactive_chat(tester: KoAlpacaModelTester):
    """ëŒ€í™”í˜• ì±—ë´‡ ëª¨ë“œ"""
    print("\nğŸ¤– KoAlpaca 5.8B ëŒ€í™”í˜• ì±—ë´‡ ì‹œì‘!")
    print("ğŸ’¡ 'quit', 'exit', 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.")
    print("ğŸ’¡ 'clear'ë¥¼ ì…ë ¥í•˜ë©´ GPU ë©”ëª¨ë¦¬ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.")
    print("="*60)

    while True:
        try:
            user_input = input("\nğŸ‘¤ You: ").strip()
            if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                print("ğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            if user_input.lower() == 'clear':
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    logger.info("ğŸ§¹ GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
                continue
            if not user_input:
                continue

            print("ğŸ¤– Bot: (ì‘ë‹µ ìƒì„± ì¤‘...)", end="\r", flush=True)
            
            answer = tester.generate_response(user_input)
            
            # ì‘ë‹µ ì¶œë ¥
            print("ğŸ¤– Bot: " + " " * 20)  # "ì‘ë‹µ ìƒì„± ì¤‘..." ë©”ì‹œì§€ ë®ì–´ì“°ê¸°
            print(answer)
            
            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í‘œì‹œ
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1024**3
                print(f"ğŸ”¥ GPU ë©”ëª¨ë¦¬: {memory_used:.2f} GB")

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            logger.error(f"ëŒ€í™” ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

def run_benchmark(tester: KoAlpacaModelTester):
    """ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    print("\nğŸ“Š KoAlpaca 5.8B ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘!")
    print("="*60)

    test_cases = [
        ("ì¸ì‚¬", "ì•ˆë…•í•˜ì„¸ìš”! ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”."),
        ("ì¼ë°˜ ì§€ì‹", "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ì™€ ì¸êµ¬ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”."),
        ("ì„¤ëª… ìš”ì²­", "ì¸ê³µì§€ëŠ¥ì´ë€ ë¬´ì—‡ì¸ì§€ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."),
        ("ì°½ì‘ ìš”ì²­", "ë´„ì— ëŒ€í•œ ì§§ì€ ì‹œë¥¼ ì¨ì£¼ì„¸ìš”."),
        ("ë¬¸ì œ í•´ê²°", "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”."),
    ]
    
    total_time = 0
    total_tokens = 0
    successful_tests = 0

    for i, (test_name, prompt) in enumerate(test_cases, 1):
        print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸ {i}/{len(test_cases)}: {test_name}")
        print(f"   í”„ë¡¬í”„íŠ¸: {prompt}")
        
        try:
            start_time = time.time()
            answer = tester.generate_response(prompt, max_new_tokens=128)
            end_time = time.time()
            
            elapsed = end_time - start_time
            total_time += elapsed
            successful_tests += 1
            
            # í† í° ìˆ˜ ê³„ì‚°
            tokens = len(tester.tokenizer.encode(answer)) if tester.tokenizer else len(answer.split())
            total_tokens += tokens
            
            print(f"   ì‘ë‹µ: {answer[:100]}{'...' if len(answer) > 100 else ''}")
            print(f"   â±ï¸  ì‹œê°„: {elapsed:.2f}ì´ˆ")
            print(f"   ğŸ“ í† í°: {tokens}ê°œ")
            print(f"   ğŸš€ í† í°/ì´ˆ: {tokens/elapsed:.1f}")

        except Exception as e:
            print(f"   âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            logger.error(f"ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ {i} ì‹¤íŒ¨: {e}")
            
    # ê²°ê³¼ ìš”ì•½
    print("\nğŸ“ˆ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼:")
    print(f"   ì„±ê³µí•œ í…ŒìŠ¤íŠ¸: {successful_tests}/{len(test_cases)}")
    if successful_tests > 0:
        avg_time = total_time / successful_tests
        avg_tokens_per_sec = total_tokens / total_time if total_time > 0 else 0
        print(f"   ì´ ì‹œê°„: {total_time:.2f}ì´ˆ")
        print(f"   í‰ê·  ì‘ë‹µ ì‹œê°„: {avg_time:.2f}ì´ˆ/ìš”ì²­")
        print(f"   ì´ ìƒì„± í† í°: {total_tokens}ê°œ")
        print(f"   í‰ê·  ìƒì„± ì†ë„: {avg_tokens_per_sec:.1f} í† í°/ì´ˆ")
        
        # RTX 5060 ì„±ëŠ¥ í‰ê°€
        if avg_tokens_per_sec > 15:
            print("   ğŸš€ ì„±ëŠ¥: ìš°ìˆ˜ (RTX 5060 ìµœì í™” ì„±ê³µ)")
        elif avg_tokens_per_sec > 10:
            print("   âš¡ ì„±ëŠ¥: ì–‘í˜¸")
        else:
            print("   ğŸŒ ì„±ëŠ¥: ê°œì„  í•„ìš” (ë©”ëª¨ë¦¬/ì„¤ì • í™•ì¸)")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("="*60)
    print("  KoAlpaca 5.8B ëª¨ë¸ í…ŒìŠ¤íŠ¸ í´ë¼ì´ì–¸íŠ¸ (RTX 5060 ìµœì í™”)")
    print("="*60)
    
    tester = KoAlpacaModelTester()
    
    # GPU ì •ë³´ í™•ì¸
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        logger.info(f"ğŸ”¥ GPU: {gpu_name}")
        logger.info(f"ğŸ”¥ GPU ë©”ëª¨ë¦¬: {gpu_memory:.1f} GB")
    else:
        logger.warning("âš ï¸ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    
    # ëª¨ë¸ ì—°ê²° í…ŒìŠ¤íŠ¸
    if not test_model_connection(tester):
        print("\nâŒ ëª¨ë¸ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ 'python download_model.py'ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return

    print("\nì‹¤í–‰í•  ì‘ì—…ì„ ì„ íƒí•˜ì„¸ìš”:")
    print("1. ëŒ€í™”í˜• ì±—ë´‡ (Interactive Chat)")
    print("2. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (Benchmark)")
    print("3. ë‹¨ì¼ í…ŒìŠ¤íŠ¸ (Single Test)")
    choice = input("\nì„ íƒ (1-3, ê¸°ë³¸ê°’: 1): ").strip()

    try:
        if choice == "2":
            run_benchmark(tester)
        elif choice == "3":
            prompt = input("í…ŒìŠ¤íŠ¸í•  ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            if prompt:
                print("\nğŸ¤– ì‘ë‹µ:")
                answer = tester.generate_response(prompt)
                print(answer)
        else:
            run_interactive_chat(tester)
    
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ í”„ë¡œê·¸ë¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    finally:
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

if __name__ == "__main__":
    main()
