#!/usr/bin/env python3
# test_trained_model.py - í›ˆë ¨ëœ LoRA ëª¨ë¸ í…ŒìŠ¤íŠ¸

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
from pathlib import Path

# ê²½ë¡œ ì„¤ì •
BASE_MODEL_PATH = Path(__file__).parent / "models" / "beomi/KoAlpaca-Polyglot-5.8B"
LORA_ADAPTER_PATH = "./lora_results_rtx5060/final_lora_adapter"

def load_model():
    """í›ˆë ¨ëœ LoRA ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    print("ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...")
    
    # 4bit quantization ì„¤ì •
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )
    
    # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        device_map="auto",
        trust_remote_code=True,
        quantization_config=bnb_config
    )
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"
    
    # LoRA ì–´ëŒ‘í„° ë¡œë“œ
    model = PeftModel.from_pretrained(model, LORA_ADAPTER_PATH)
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    
    return model, tokenizer

def generate_response(model, tokenizer, prompt, max_length=512):
    """ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    # í† í¬ë‚˜ì´ì¦ˆ (token_type_ids ì œì™¸)
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        truncation=True, 
        max_length=2048,
        return_token_type_ids=False
    )
    
    # GPUë¡œ ì´ë™
    if torch.cuda.is_available():
        inputs = {k: v.cuda() for k, v in inputs.items()}
    
    # ìƒì„±
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    # ë””ì½”ë”©
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì œê±°í•˜ê³  ì‘ë‹µ ë¶€ë¶„ë§Œ ë°˜í™˜
    response = response[len(prompt):].strip()
    
    return response

def main():
    print("=" * 60)
    print("      í›ˆë ¨ëœ LoRA ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer = load_model()
    
    # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ (í›ˆë ¨ ë°ì´í„°ì™€ ìœ ì‚¬í•œ í˜•ì‹)
    test_prompt = """You are an expert ESG report writer. Based on the following structured data, synthesize the information into a single, cohesive, and professional Korean paragraph for a sustainability report.

### Data:
{
  "company_info": {
    "name": "í…ŒìŠ¤íŠ¸ê¸°ì—…",
    "id": "company_test"
  },
  "g_standard": "GRI 2: General Disclosures 2021",
  "disclosure_item": "2-1 ì¡°ì§ ì„¸ë¶€ ì •ë³´",
  "requirements_and_data": [
    {
      "id": "gri2-1-a",
      "question": "a. ë²•ì  ëª…ì¹­ ë³´ê³ í•´ì£¼ì„¸ìš”.",
      "raw_answer": "ë²•ì  ëª…ì¹­: í…ŒìŠ¤íŠ¸ê¸°ì—… ì£¼ì‹íšŒì‚¬"
    },
    {
      "id": "gri2-1-b", 
      "question": "b. ì†Œìœ ê¶Œ ë° ë²•ì¸ êµ¬ë¶„ ë³´ê³ í•´ì£¼ì„¸ìš”.",
      "raw_answer": "ì£¼ì‹íšŒì‚¬"
    },
    {
      "id": "gri2-1-c",
      "question": "c. ë³¸ì‚¬ ìœ„ì¹˜ ë³´ê³ í•´ì£¼ì„¸ìš”.",
      "raw_answer": "ë³¸ì‚¬ ìœ„ì¹˜: ëŒ€í•œë¯¼êµ­ ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬"
    },
    {
      "id": "gri2-1-d",
      "question": "d. ìš´ì˜ êµ­ê°€(ë“¤) ë³´ê³ í•´ì£¼ì„¸ìš”.",
      "raw_answer": "ëŒ€í•œë¯¼êµ­, ë¯¸êµ­"
    }
  ]
}

### Polished Report Paragraph:"""

    print("\nğŸ“ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸:")
    print("-" * 40)
    print(test_prompt)
    print("-" * 40)
    
    print("\nğŸ¤– ëª¨ë¸ ì‘ë‹µ ìƒì„± ì¤‘...")
    response = generate_response(model, tokenizer, test_prompt)
    
    print("\nğŸ“Š ìƒì„±ëœ ESG ë³´ê³ ì„œ:")
    print("=" * 40)
    print(response)
    print("=" * 40)
    
    # ì¶”ê°€ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ
    print("\nğŸ’¡ ì¶”ê°€ í…ŒìŠ¤íŠ¸ë¥¼ ì›í•˜ì‹œë©´ í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: 'quit'):")
    
    while True:
        user_input = input("\ní”„ë¡¬í”„íŠ¸: ").strip()
        if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
            break
        
        if user_input:
            print("\nğŸ¤– ì‘ë‹µ ìƒì„± ì¤‘...")
            response = generate_response(model, tokenizer, user_input)
            print(f"\nğŸ“ ì‘ë‹µ:\n{response}")

if __name__ == "__main__":
    main() 