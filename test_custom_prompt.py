#!/usr/bin/env python3
# test_custom_prompt.py - íŠ¹ì • í”„ë¡¬í”„íŠ¸ë¡œ í›ˆë ¨ëœ LoRA ëª¨ë¸ í…ŒìŠ¤íŠ¸

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
from pathlib import Path

# ê²½ë¡œ ì„¤ì •
BASE_MODEL_PATH = Path(__file__).parent / "models" / "beomi/KoAlpaca-Polyglot-5.8B"
LORA_ADAPTER_PATH = "./lora_corrected/final_lora_adapter"  # ìƒˆë¡œ í›ˆë ¨ëœ ëª¨ë¸

def load_model():
    """í›ˆë ¨ëœ LoRA ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    print("ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...")
    
    # 4bit quantization ì„¤ì •
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )
    
    # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        device_map="auto",
        trust_remote_code=True,
        quantization_config=bnb_config
    )
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"
    
    # LoRA ì–´ëŒ‘í„° ë¡œë“œ
    model = PeftModel.from_pretrained(model, LORA_ADAPTER_PATH)
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    
    return model, tokenizer

def generate_response(model, tokenizer, prompt, max_length=512):
    """ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    # í† í¬ë‚˜ì´ì¦ˆ (token_type_ids ì œì™¸)
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        truncation=True, 
        max_length=2048,
        return_token_type_ids=False
    )
    
    # GPUë¡œ ì´ë™
    if torch.cuda.is_available():
        inputs = {k: v.cuda() for k, v in inputs.items()}
    
    # ìƒì„±
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.5,
            top_p=0.95,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
            no_repeat_ngram_size=3,
        )
    
    # ë””ì½”ë”©
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì œê±°í•˜ê³  ì‘ë‹µ ë¶€ë¶„ë§Œ ë°˜í™˜
    response = response[len(prompt):].strip()
    
    return response

def main():
    print("=" * 80)
    print("      ìƒˆë¡œ í›ˆë ¨ëœ LoRA ëª¨ë¸ í…ŒìŠ¤íŠ¸ (116ê°œ ìƒ˜í”Œ)")
    print("=" * 80)
    
    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer = load_model()
    
    # ì‚¬ìš©ìê°€ ì œê³µí•œ ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸ (GRI 414)
    custom_prompt = """You are an expert ESG report writer. Based on the following structured data, synthesize the information into a single, cohesive, and professional Korean paragraph for a sustainability report.

### Data:
{
  "company_info": {
    "name": "xxê¸°ì—…",
    "id": "company_01"
  },
  "g_standard": "GRI 414: Supplier Social Assessment 2016",
  "disclosure_item": "414-1 ì‚¬íšŒì  ê¸°ì¤€ì— ë”°ë¥¸ ì‹¬ì‚¬ë¥¼ ê±°ì¹œ ì‹ ê·œ ê³µê¸‰ì—…ì²´",
  "requirements_and_data": [
    {
      "id": "gri414-1-a",
      "question": "a. ì‚¬íšŒì  ê¸°ì¤€ì— ë”°ë¥¸ ì‹¬ì‚¬ë¥¼ ê±°ì¹œ ì‹ ê·œ ê³µê¸‰ì—…ì²´ë“¤ì˜ ë¹„ì¤‘ì„ ë³´ê³ í•´ì£¼ì„¸ìš”.",
      "raw_answer": "ëª¨ë“  ì‹ ê·œ ê³µê¸‰ì—…ì²´(100%)ì— ëŒ€í•´ ì¸ê¶Œ, ë…¸ë™, ì•ˆì „, ìœ¤ë¦¬ ë“± ì‚¬íšŒì  ê¸°ì¤€ì„ í¬í•¨í•œ ìê²© ì‹¬ì‚¬ë¥¼ ì‹¤ì‹œí•©ë‹ˆë‹¤."
    }
  ]
}

### Polished Report Paragraph:"""

    print("\nğŸ“ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸:")
    print("-" * 60)
    print(custom_prompt)
    print("-" * 60)
    
    print("\nğŸ¤– ëª¨ë¸ ì‘ë‹µ ìƒì„± ì¤‘...")
    response = generate_response(model, tokenizer, custom_prompt, max_length=800)
    
    print("\nğŸ“Š ìƒì„±ëœ ESG ë³´ê³ ì„œ:")
    print("=" * 60)
    print(response)
    print("=" * 60)

if __name__ == "__main__":
    main() 